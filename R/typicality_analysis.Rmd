---
title: "Analysing Typicality Effects in LMs"
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
library(tidyverse)
library(broom)
library(widyr)
library(fs)
library(glue)
library(ggrepel)
library(gt)
library(knitr)
library(kableExtra)

theme_set(theme_bw(base_size = 16))
```

## Loading Data

```{r}
model_meta <- read_csv(here::here("data/meta.csv")) %>%
  mutate(
    model = str_remove(model, "(google_)"),
    model = str_remove(model, "\\-generator"),
    model = str_remove(model, "\\-uncased"),
    model = str_replace(model, "large", "l"),
    model = str_replace(model, "medium", "m"),
    model = str_replace(model, "small", "s"),
    model = str_replace(model, "base", "b"),
    model = str_replace(model, "openai\\-", ""),
    model = str_replace(model, "\\-v1", "")
  ) 

levels = c('5gram', 'albert-b', 'albert-l', 'albert-xl', 'albert-xxl', 'distilbert-b', 'bert-b', 'bert-l', 'electra-s', 'electra-b', 'electra-l', 'distilgpt2', 'gpt', 'gpt2', 'gpt2-m', 'gpt2-l', 'gpt2-xl', 'distilroberta-b', 'roberta-b', 'roberta-l')

colors <- c('#595959', '#73a2c6','#5d8abd','#4771b2','#2e59a8','#fee391','#fec44f','#fe9929','#bcbddc', '#6a51a3','#54278f','#ff9895','#f4777f','#e4576b','#cf3759','#b41648','#93003a','#d9f0a3','#78c679','#238443')

shortlevels = c("5g", "A-b", "A-l", "A-xl", "A-xxl", "dB-b", "B-b", "B-l", "E-s", "E-b", "E-l", "dGPT2", "GPT", "GPT2", "GPT2-m", "GPT2-l", "GPT2-xl", "dR-b", "R-b", "R-l")


frequencies <- read_csv(here::here("data/frequencies/wikicounts.csv")) %>%
  rename(item = word)

typicality_ratings <- dir_ls(here::here("data/rosch1975/results/rosch1975/"), regexp = "*.csv") %>%
  map_df(read_csv) %>%
  mutate(
    model = str_remove(model, "(google_)"),
    model = str_remove(model, "\\-generator"),
    model = str_remove(model, "\\-uncased"),
    model = str_replace(model, "large", "l"),
    model = str_replace(model, "medium", "m"),
    model = str_replace(model, "small", "s"),
    model = str_replace(model, "base", "b"),
    model = str_replace(model, "openai\\-", ""),
    model = str_replace(model, "\\-v1", "")
  ) %>%
  group_by(model, category) %>% 
  mutate(rank = row_number()) %>%
  ungroup() %>%
  inner_join(read_csv(here::here("data/rosch1975/rosch1975_ratings.csv")))

ngram <- typicality_ratings %>% 
  filter(model == "5gram") %>%
  rename(ngram_score = score) %>%
  select(-stimulus, -model, -params)

induction <- dir_ls(here::here("data/results/premiseconclusion/"), regexp = "*.csv") %>%
  map_df(read_csv) %>%
  mutate(
    model = str_remove(model, "(google_)"),
    model = str_remove(model, "\\-generator"),
    model = str_remove(model, "\\-uncased"),
    model = str_replace(model, "large", "l"),
    model = str_replace(model, "medium", "m"),
    model = str_replace(model, "small", "s"),
    model = str_replace(model, "base", "b"),
    model = str_replace(model, "openai\\-", ""),
    model = str_replace(model, "\\-v1", "")
  ) %>%
  group_by(model, category, predicate_id, argument_id) %>% 
  mutate(rank = row_number()) %>%
  ungroup() %>%
  mutate(
    item = case_when(item == "brussel" ~ "brussels sprouts", TRUE ~ item)
  )

feature_overlaps <- dir_ls(here::here("data/rosch1975/results/rosch1975_features/"), regexp = "*.csv") %>%
  map_df(read_csv) %>%
  mutate(
    model = str_remove(model, "(google_)"),
    model = str_remove(model, "\\-generator"),
    model = str_remove(model, "\\-uncased"),
    model = str_replace(model, "large", "l"),
    model = str_replace(model, "medium", "m"),
    model = str_replace(model, "small", "s"),
    model = str_replace(model, "base", "b"),
    model = str_replace(model, "openai\\-", ""),
    model = str_replace(model, "\\-v1", "")
  )

ngram_fo <- feature_overlaps %>% 
  filter(model == "5gram") %>%
  rename(ngram_feature_score = score) %>%
  select(-stimulus, -model, -params)
```


# ngram correlaion

```{r}
typicality_ratings %>%
  filter(model != "5gram") %>%
  inner_join(ngram) %>%
  group_by(model, params) %>%
  nest() %>%
  mutate(
    # cor = map(data, function(x) cor.test(x$ngram_score, x$score, method = "pearson") %>% tidy())
    fit = map(data, function(x) lm(x$score ~ x$ngram_score) %>% glance())
  ) %>%
  unnest(fit) %>%
  select(-data) %>%
  # group_by(model, params) %>%
  # summarize(estimate = mean(estimate)) %>%
  ungroup() %>%
  inner_join(model_meta) %>%
  ggplot(aes(params/1e6, r.squared, color = color, group = family)) + 
  geom_point(size = 3) + 
  # geom_line() +
  geom_text_repel(aes(label = model), nudge_y = 0.02, nudge_x = 0.01, size = 5) +
  # facet_wrap(~family) +
  scale_color_identity() +
  scale_y_continuous(limits = c(0, 0.4), labels = scales::percent_format(accuracy = 1)) +
  labs(
    x = "Parameters (in millions)",
    y = "Variance Explained"
  )
```

## ngram vs performance

```{r}
ngram %>%
  group_by(category) %>%
  nest() %>%
  mutate(cor = map(data, function(x) cor.test(x$rating, -x$ngram_score, method = "kendall") %>% tidy())) %>%
  unnest(cor) %>%
  select(-data)
```

## Qual

```{r}
combined <- typicality_ratings %>% 
  filter(model != "5gram") %>% 
  group_by(model) %>% 
  mutate(score = (score - min(score))/(max(score) - min(score))) %>% 
  ungroup() %>% group_by(item, category) %>% 
  summarize(score = mean(score), rating = mean(rating), rank = mean(rank))

combined %>%
  group_by(category) %>%
  filter(rating == min(rating))
```


## Category wise logprob vs Human judgements

```{r}
typicality_ratings %>%
  # filter(model != "5gram") %>%
  group_by(model, params, category) %>%
  nest() %>%
  mutate(cor = map(data, function(x) cor.test(x$rating, -x$score, method = "spearman") %>% tidy())) %>%
  unnest(cor) %>%
  select(-data) %>%
  inner_join(model_meta) %>%
  mutate(model = factor(model, levels = levels)) %>%
  ggplot(aes(model, estimate, color = color, fill = color)) +
  geom_col() +
  # geom_text(aes(y = 0.02, label = model), color = "white", angle = 90, hjust = "left", vjust = "center") +
  facet_wrap(~category, nrow = 2) +
  scale_color_identity(guide = "legend", name = "Model", aesthetics = c("color", "fill"), labels = levels, breaks = colors) +
  # scale_fill_identity() +
  scale_y_continuous(limits = c(-0.4, 1.0)) +
  labs(
    x = "Model",
    y = "Kendall's Tau"
  ) +
  theme(
    legend.position = "top",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.x = element_blank()
  )
```

## Overall logprob vs Human judgements

```{r}
typicality_ratings %>%
  # filter(model != "5gram") %>%
  group_by(model, params) %>%
  nest() %>%
  mutate(cor = map(data, function(x) cor.test(x$rating, -x$score, method = "pearson") %>% tidy())) %>%
  unnest(cor) %>%
  select(-data) %>%
  inner_join(model_meta) %>%
  mutate(model = factor(model, levels = levels), short = factor(short, levels = shortlevels)) %>%
  ggplot(aes(short, estimate, color = color, fill = color)) +
  geom_col() +
  # geom_text(aes(y = 0.02, label = model), color = "white", angle = 90, hjust = "left", vjust = "center") +
  # facet_wrap(~category, nrow = 2) +
  # scale_color_identity(guide = "legend", name = "Model", aesthetics = c("color", "fill"), labels = levels, breaks = colors) +
  annotate("text", x = 3.5, y = 0.5, label = "ALBERT", size = 5.5, color = "#2e59a8", fontface = "bold") +
  annotate("text", x = 7, y = 0.5, label = "BERT", size = 5.5, color = "#fe9929", fontface = "bold") +
  annotate("text", x = 10, y = 0.5, label = "ELECTRA", size = 5.5, color = "#54278f", fontface = "bold") +
  annotate("text", x = 14.5, y = 0.5, label = "GPT/GPT2", size = 5.5, color = "#93003a", fontface = "bold") +
  annotate("text", x = 19, y = 0.5, label = "RoBERTa", size = 5.5, color = "#238443", fontface = "bold") +
  scale_color_identity(aesthetics = c("color", "fill")) +
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0.008)) +
  labs(
    x = "Model",
    y = "Pearson's r"
  ) +
  theme_bw(base_size = 18) +
  theme(
    legend.position = "top",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_text(angle = 20, vjust = 0.8),
    axis.title.x = element_blank(),
    plot.margin = margin(0.1, 0.1, 0.1, 0.1, "cm"),
    panel.background = element_rect(fill = "transparent"), # bg of the panel
    plot.background = element_rect(fill = "transparent", color = NA), # bg of the plot
    legend.background = element_rect(fill = "transparent"), # get rid of legend bg
    legend.box.background = element_rect(fill = "transparent")
  )

# ggsave("paper/taxonomicverification.pdf", height = 5, width = 10, bg = "transparent")
ggsave("paper/taxonomicverification01.pdf", height = 5.2, width = 8.2, bg = "transparent")
```



```{r}
feature_overlaps %>%
  filter(model != "5gram") %>%
  inner_join(ngram_fo) %>%
  group_by(model, params) %>%
  nest() %>%
  mutate(
    # cor = map(data, function(x) cor.test(x$ngram_score, x$score, method = "pearson") %>% tidy())
    fit = map(data, function(x) lm(x$score ~ x$ngram_feature_score) %>% glance())
  ) %>%
  unnest(fit) %>%
  select(-data) %>%
  # group_by(model, params) %>%
  # summarize(estimate = mean(estimate)) %>%
  ungroup() %>%
  inner_join(model_meta) %>%
  ggplot(aes(params/1e6, r.squared, color = color, group = family)) + 
  geom_point(size = 3) + 
  # geom_line() +
  geom_text_repel(aes(label = model), nudge_y = 0.02, nudge_x = 0.01, size = 5) +
  # facet_wrap(~family) +
  scale_color_identity() +
  scale_y_continuous(limits = c(0, 1), labels = scales::percent_format(accuracy = 1)) +
  labs(
    x = "Parameters (in millions)",
    y = "Variance Explained"
  )
```


## How do typicality judgements of models align with frequencies?

```{r}
typicality_ratings %>%
  inner_join(frequencies) %>%
  group_by(model, params, category) %>%
  nest() %>%
  mutate(
    cor = map(data, function(x) cor.test(x$count, x$score, method = "kendall") %>% tidy())
  ) %>%
  unnest(cor) %>%
  select(-data) %>%
  group_by(model, params) %>%
  summarize(estimate = mean(estimate)) %>%
  ungroup() %>%
  inner_join(model_meta) %>%
  ggplot(aes(params/1e6, estimate, color = color)) + 
  geom_point(size = 3) +
  geom_text_repel(aes(label = model), nudge_y = 0.01, size = 5) +
  scale_color_identity() +
  # scale_alpha_identity() +
  # geom_smooth(method = "lm") +
  # scale_x_log10() +
  # scale_y_continuous(limits = c(0.2, 0.5)) +
  labs(
    x = "Parameters (in millions)",
    y = "Kendall's Tau"
  )
```

Seems like models' reliance on frequency statistics decreases with increase in the number of parameters.

## Systematicity of models at the property level

Measure correlation between typicality ratings elicited by model and its predictions for whether the given item has the given property that is characteristic for the category.

```{r, include=FALSE}
# feature_overlaps %>%
#   filter(item != "sprouts") %>%
#   inner_join(typicality_ratings %>% select(model, params, category, item, typicality_lp = logprob, typicality_nlp = normalized_logprob, rank)) %>%
#   group_by(model, params, category, feature) %>%
#   nest() %>%
#   mutate(
#     cor_lp = map(data, function(x) {cor.test(x$logprob, x$typicality_lp, method = "kendall") %>% tidy()}),
#     # cor_nlp = map(data, function(x) {cor.test(x$logprob, x$typicality_nlp, method = "kendall") %>% tidy()})
#   ) %>%
#   unnest(cor_lp) %>%
#   group_by(model, params, category) %>%
#   summarize(
#     correlation = mean(estimate)
#   ) %>%
#   ungroup() %>%
#   inner_join(model_meta) %>%
#   mutate(model = factor(model, levels = levels)) %>%
#   ggplot(aes(model, correlation, color = color, fill = color)) +
#   geom_col() +
#   facet_wrap(~category) +
#   scale_color_identity() +
#   scale_fill_identity()
```


```{r}
feature_overlaps %>%
    filter(item != "sprouts") %>%
    inner_join(typicality_ratings %>% select(model, params, category, item, typicality = score, rank)) %>% 
  group_by(model, params, category, feature) %>% 
  mutate(
    standardized_logprob = (score - min(score))/(max(score) - min(score))
  ) %>%
  ungroup() %>%
  group_by(model, params, category, item) %>%
  summarise(
    property_score = mean(standardized_logprob),
    typicality = mean(typicality)
  ) %>%
  ungroup() %>%
  group_by(model, params, category) %>%
  nest() %>%
  mutate(
    cor_lp = map(data, function(x) {cor.test(x$property_score, x$typicality, method = "kendall") %>% tidy()}),
    # cor_nlp = map(data, function(x) {cor.test(x$logprob, x$typicality_nlp, method = "kendall") %>% tidy()})
  ) %>%
  unnest(cor_lp) %>%
  select(-data) %>%
  inner_join(model_meta) %>%
  mutate(model = factor(model, levels = levels)) %>%
  ggplot(aes(model, estimate, color = color, fill = color)) +
  geom_col() +
  geom_text(aes(y = 0.02, label = model), color = "white", angle = 90, hjust = "left", vjust = "center") +
  facet_wrap(~category) +
  scale_color_identity() +
  scale_fill_identity() +
  scale_y_continuous(limits = c(0, 1), expand = c(0,0)) +
  labs(
    x = "Model",
    y = "Kendall's Tau"
  ) +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.x = element_blank()
  )
```


## Do LMs more readily extend new information about typical items to all members of a category?

```{r}
induction_caveats <- induction %>%
  filter(model != "5gram") %>%
  group_by(model, params) %>%
  summarize(
    conclusion_only = mean(score > conclusion_only),
    order_sensitivity = mean(shuffled_diff > 0),
    taxonomic_priming = mean(score > control_score)
  ) %>%
  ungroup() %>%
  inner_join(model_meta) %>%
  mutate(name = factor(model, levels = levels))
  # mutate_at(vars(-params, -model), round, digits = 4) %>%
  # gt() %>%
  # fmt_percent(columns = vars(conclusion_only, order_sensitivity, taxonomic_priming), decimals = 2)

induction_caveats %>%
  group_by(family) %>%
  summarize(
    TS_mean = mean(taxonomic_priming),
    TS_sd = sd(taxonomic_priming),
    POS_mean = mean(order_sensitivity),
    POS_sd = sd(order_sensitivity)
  ) %>%
  ungroup() %>%
  transmute(
    family,
    TS = glue("{round(TS_mean, 2)} pm {round(TS_sd, 2)}"),
    POS = glue("{round(POS_mean, 2)} pm {round(POS_sd, 2)}")
  ) %>%
  kable("latex")


family_caveats <- induction %>%
  filter(model != "5gram") %>%
  inner_join(model_meta) %>%
  group_by(family) %>%
  summarize(
    conclusion_only = mean(score > conclusion_only),
    order_sensitivity = mean(shuffled_diff > 0),
    taxonomic_priming = mean(score > control_score)
  ) %>%
  ungroup() 

family_caveats %>%
  select(family, POS = order_sensitivity, TS = taxonomic_priming) %>%
  kable("latex")
```


```{r}
induction_caveats %>%
  ggplot(aes(params/1e6, order_sensitivity, color = color)) +
  geom_point(size = 2) +
  # geom_line() +
  scale_color_identity() + 
  scale_y_continuous(limits = c(0.5, 1), labels = scales::percent_format()) +
  geom_text_repel(aes(label = model), nudge_y = 0.01, size = 5) + 
  labs(
    x = "Parameters (in millions)",
    y = "Premise Order Sensitivity"
  ) +
  theme(
    panel.background = element_rect(fill = "transparent"), # bg of the panel
    plot.background = element_rect(fill = "transparent", color = NA), # bg of the plot
    legend.background = element_rect(fill = "transparent"), # get rid of legend bg
    legend.box.background = element_rect(fill = "transparent")
  )

ggsave("paper/posensitivity.pdf", height = 5, width = 5.5, bg = "transparent")
```


```{r}
induction_caveats %>%
  ggplot(aes(params/1e6, taxonomic_priming, color = color)) +
  geom_point(size = 2) +
  # geom_line() +
  scale_color_identity() + 
  scale_y_continuous(limits = c(0.5, 1), labels = scales::percent_format()) +
  geom_text_repel(aes(label = model), size = 5) +
  labs(
    x = "Parameters (in millions)",
    y = "Taxonomic Sensitivity"
  ) +
  theme(
    panel.background = element_rect(fill = "transparent"), # bg of the panel
    plot.background = element_rect(fill = "transparent", color = NA), # bg of the plot
    legend.background = element_rect(fill = "transparent"), # get rid of legend bg
    legend.box.background = element_rect(fill = "transparent")
  )

ggsave("paper/taxonomicsensitivity.pdf", height = 5, width = 5.5, bg = "transparent")
```

```{r}
induction %>%
  # filter(model != "5gram") %>%
  inner_join(typicality_ratings %>% select(model, params, category, typicality = score, rank, rating), by = c("model", "params", "category", "rank")) %>%
  group_by(model, params, category, predicate_id, argument_id) %>%
  mutate(
    # score = score - control_score,
    standardized_logprob = (score - min(score))/(max(score) - min(score))
  ) %>%
  ungroup() %>%
  group_by(model, params, category, item) %>%
  summarise(
    standardized_logprob = mean(standardized_logprob),
    typicality = mean(typicality),
    rank = mean(rank),
    rating = mean(rating)
  ) %>%
  ungroup() %>%
  # group_by(model, params, category) %>%
  group_by(model, params) %>%
  nest() %>%
  mutate(
    # cor_lp = map(data, function(x) lm(rating ~ standardized_logprob + factor(category), data = x) %>% glance())
    cor_lp = map(data, function(x) {cor.test(-x$standardized_logprob, x$rating, method = "pearson") %>% tidy()}),
    # cor_nlp = map(data, function(x) {cor.test(x$logprob, x$typicality_nlp, method = "kendall") %>% tidy()})
  ) %>%
  unnest(cor_lp) %>%
  select(-data) %>%
  inner_join(model_meta) %>%
  mutate(model = factor(model, levels = levels)) %>%
  # ggplot(aes(model, r.squared, color = color, fill = color)) +
  ggplot(aes(model, estimate, color = color, fill = color)) +
  geom_col() +
  # geom_text(aes(y = 0.95, label = model), color = "black", angle = 90, hjust = "right", vjust = "center") +
  # facet_wrap(~category) +
  scale_color_identity(guide = "legend", name = "Model", aesthetics = c("color", "fill"), labels = levels, breaks = colors) +
  # scale_fill_identity() +
  scale_y_continuous(limits = c(0, 0.5), expand = c(0,0)) +
  labs(
    x = "Model",
    y = "Pearson's r"
  ) +
  theme(
    legend.position = "top",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.x = element_blank()
  )
```

## Taxonomic Sentence Verification (alternate way of testing):
```{r}
experiment_results_typicality <- typicality_ratings %>%
  # filter(model != "5gram") %>%
  # mutate(score = score - control_score) %>%
  select(model, item, category, score) 

human_groups_typicality <- typicality_ratings %>%
  distinct(item, category, rank) %>%
  group_by(category) %>%
  nest() %>%
  mutate(
    data = map(data, function(x) {
      if(nrow(x) %% 2 != 0) {
        return(x[-nrow(x),])
      }
      else {
        return(x)
      }
    })
  ) %>%
  unnest(data) %>%
  add_count(category) %>%
  group_by() %>%
  mutate(
    group = case_when(
      rank <= n/2 ~ "typical",
      TRUE ~ "atypical"
    )
  )

pairing_human_typicality <- human_groups_typicality %>%
  filter(group == "typical") %>%
  select(category, item1 = item) %>%
  group_by(category) %>%
  nest(item1 = -group_cols()) %>%
  inner_join(
    human_groups_typicality %>%
      filter(group == "atypical") %>%
      select(category, item2 = item) %>%
      group_by(category) %>%
      nest(item2 = -group_cols())
  )

run_pairing_taxonomic <- function() {
  result <- pairing_human_typicality %>%
    mutate(
        item2 = map(item2, function(x) {
            rows = sample(nrow(x))
            return(x[rows, ])
        })
    ) %>%
    unnest(c(item1, item2)) %>% 
    ungroup() %>%
    inner_join(experiment_results_typicality %>% select(model, category, item1 = item, score1 = score)) %>%
    inner_join(experiment_results_typicality %>% select(model, category, item2 = item, score2 = score))

  return(result)
}
```

```{r}
set.seed(1234)
# this will be fun.
replicates_human_taxonomic <- replicate(100, run_pairing_taxonomic(), simplify = FALSE)
```

## Taxonomic Sentence Verification.

```{r}
replicates_human_taxonomic %>%
  map(
    function(x) {
      x %>% group_by(model) %>%
      summarize(
        alignment = mean(score1 > score2)
      )
    }
  ) %>%
  bind_rows(.id = "run") %>%
  group_by(model) %>%
  summarize(
    se = 1.96 * plotrix::std.error(alignment), 
    alignment = mean(alignment)
  ) %>%
  inner_join(model_meta) %>%
  mutate(model = factor(model, levels = levels)) %>%
  ggplot(aes(model, alignment, color = color, fill = color)) +
  geom_col() +
  geom_hline(yintercept = 0.5, linetype = 2) +
  # geom_text(aes(y = 0.95, label = model), color = "black", angle = 90, hjust = "right", vjust = "center") +
  # facet_wrap(~category) +
  # scale_color_identity(guide = "legend", name = "Model", aesthetics = c("color", "fill"), labels = levels, breaks = colors) +
  scale_color_identity(aesthetics = c("color", "fill")) +
  scale_y_continuous(limits = c(0, 1.0), labels = scales::percent_format(), expand = c(0,0.01)) +
  labs(
    x = "Model",
    y = "Premise Typicality"
  ) +
  theme(
    legend.position = "top",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_text(angle = 20, vjust = 0.8),
    panel.background = element_rect(fill = "transparent"), # bg of the panel
    plot.background = element_rect(fill = "transparent", color = NA), # bg of the plot
    legend.background = element_rect(fill = "transparent"), # get rid of legend bg
    legend.box.background = element_rect(fill = "transparent")
  )
```


## Category based Induction (10000 times)


## Correcting model results by regressing out controls

```{r correcting}
test <- induction %>%
  filter(predicate_id == 1, argument_id == 2, category == "bird") %>%
  mutate(shuffled_diff2 = score - shuffled_diff)

test_fit <- lm(score ~ scale(control_score, scale = FALSE) + scale(shuffled_diff, scale = FALSE), data = induction)
summary(test_fit)
residuals(test_fit) + coef(test_fit)["(Intercept)"]

corrected <- induction %>%
  # mutate(shuffled_diff = score - shuffled_diff) %>%
  group_by(model, predicate_id, argument_id, category) %>% mutate(item_id = row_number()) %>% ungroup() %>%
  group_by(model, predicate_id, argument_id, category) %>%
  nest() %>%
  mutate(
    corrected_score = map(data, function(x) {
      fit <- lm(score ~ scale(control_score, scale = FALSE) + scale(shuffled_diff, scale = FALSE), data = x)
      # fit <- lm(score ~ control_score + shuffled_diff, data = x)
      corrected_score = residuals(fit) + coef(fit)["(Intercept)"]
      p_value = glance(fit)$p.value
      tidied <- tidy(fit)
      beta_1 = tidied %>% filter(term == "scale(control_score, scale = FALSE)") %>% pull(estimate)
      # beta_1 = tidied %>% filter(term == "control_score") %>% pull(estimate)
      beta_2 = tidied %>% filter(term == "scale(shuffled_diff, scale = FALSE)") %>% pull(estimate)
      # beta_2 = tidied %>% filter(term == "shuffled_diff") %>% pull(estimate)
      return(
        tibble(item = x$item, item_id = x$item_id, score = x$score, corrected_score = corrected_score, rsq = summary(fit)$r.squared, p = p_value, beta1 = beta_1, beta2 = beta_2)
      )
    })
  ) %>%
  select(-data) %>%
  unnest(corrected_score)

corrected %>%
  inner_join(typicality_ratings %>% select(model, category, model_score = score, rating) %>% group_by(model, category) %>% mutate(item_id = row_number())) %>%
  group_by(model, category, predicate_id, argument_id) %>%
  mutate(
    model_score = (model_score - min(model_score))/(max(model_score) - min(model_score)),
    score = (score - min(score))/(max(score) - min(score)),
    corrected_score = (corrected_score - min(corrected_score))/(max(corrected_score) - min(corrected_score))
  ) %>%
  ungroup() %>%
  group_by(model, category, item, item_id) %>%
  summarize(
    taxonomic = mean(model_score),
    corrected_score = mean(corrected_score),
    score = mean(score),
    rating = mean(rating)
  ) %>% group_by(model) %>%
  nest() %>%
  mutate(
    cor = map(data, function(x) {
      pre = cor.test(-x$score, x$rating) %>% 
        tidy()
      post = cor.test(-x$corrected_score, x$rating) %>% 
        tidy()
      taxonomic = cor.test(x$corrected_score, x$taxonomic) %>% 
        tidy()
      return(
        tibble(
          pre_cor = pre$estimate,
          pre_p = pre$p.value,
          post_cor = post$estimate,
          post_p = post$p.value,
          consistency = taxonomic$estimate,
          consistency_p = taxonomic$p.value
        )
      )
    })
  ) %>%
  select(-data) %>%
  unnest(cor) %>%
  inner_join(model_meta) %>% 
  mutate(model = factor(model, levels = levels), short = factor(short, levels = shortlevels)) %>%
  ggplot(aes(short, consistency, color = color, fill = color)) +
  geom_col() +
  scale_color_identity(aesthetics = c("color", "fill")) +
  annotate("text", x = 2.5, y = 0.5, label = "ALBERT", size = 5.5, color = "#2e59a8", fontface = "bold") +
  annotate("text", x = 6, y = 0.5, label = "BERT", size = 5.5, color = "#fe9929", fontface = "bold") +
  annotate("text", x = 9, y = 0.5, label = "ELECTRA", size = 5.5, color = "#54278f", fontface = "bold") +
  annotate("text", x = 13.5, y = 0.5, label = "GPT/GPT2", size = 5.5, color = "#93003a", fontface = "bold") +
  annotate("text", x = 18, y = 0.5, label = "RoBERTa", size = 5.5, color = "#238443", fontface = "bold") +
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0.008)) +
  labs(
    y = "Pearson's r"
  ) +
  theme_bw(base_size = 18) +
  theme(
    legend.position = "top",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_text(angle = 20, vjust = 0.8),
    axis.title.x = element_blank(),
    plot.margin = margin(0.1, 0.1, 0.1, 0.1, "cm"),
    panel.background = element_rect(fill = "transparent"), # bg of the panel
    plot.background = element_rect(fill = "transparent", color = NA), # bg of the plot
    legend.background = element_rect(fill = "transparent"), # get rid of legend bg
    legend.box.background = element_rect(fill = "transparent")
  )
  # scale_y_continuous(limits = c(0, 1), expand = c(0, 0.01)) +
  # labs(
  #   y = "Pearson's r"
  # ) +
  # theme(
  #   panel.grid.major = element_blank(),
  #   panel.grid.minor = element_blank(),
  #   axis.text.x = element_text(angle = 20, vjust = 0.8),
  #   axis.title.x = element_blank(),
  #   plot.margin = margin(0.1, 0, 0, 0, "cm"),
  #   panel.background = element_rect(fill = "transparent"), # bg of the panel
  #   plot.background = element_rect(fill = "transparent", color = NA), # bg of the plot
  #   legend.background = element_rect(fill = "transparent"), # get rid of legend bg
  #   legend.box.background = element_rect(fill = "transparent")
  # )

# ggsave(here::here("paper/correctedinduction.pdf"), height = 5, width = 10)
ggsave(here::here("paper/correctedinduction01.pdf"), height = 5.2, width = 8.2)
```


```{r}
experiment_results <- induction %>%
  filter(model != "5gram") %>%
  mutate(score = score) %>% # comment this later lol.
  select(model, category, predicate_id, argument_id, item, rank, strength = score) %>%
  inner_join(
    typicality_ratings %>%
      filter(model != "5gram") %>%
      group_by(model, category) %>%
      mutate(typicality_rank = rank(-score)) %>%
      select(model, category, rank, typicality = score, typicality_rank)
  )
experiment_results <- corrected %>%
  inner_join(typicality_ratings %>% select(model, category, typicality = score, rating) %>% group_by(model, category) %>% mutate(item_id = row_number())) %>%
  ungroup() %>%
  rename(rank = item_id)

human_groups <- experiment_results %>%
  distinct(item, category, predicate_id, argument_id, rank) %>%
  group_by(category, predicate_id, argument_id) %>%
  nest() %>%
  mutate(
    data = map(data, function(x) {
      if(nrow(x) %% 2 != 0) {
        return(x[-nrow(x),])
      }
      else {
        return(x)
      }
    })
  ) %>%
  unnest(data) %>%
  add_count(category, predicate_id, argument_id) %>%
  group_by() %>%
  mutate(
    group = case_when(
      rank <= n/2 ~ "typical",
      TRUE ~ "atypical"
    )
  )

model_groups <- experiment_results %>%
  distinct(model, item, category, predicate_id, argument_id, typicality_rank, strength) %>%
  group_by(model, category, predicate_id, argument_id) %>%
  nest() %>%
  mutate(
    data = map(data, function(x) {
      x = x %>% arrange(typicality_rank)
      if(nrow(x) %% 2 != 0) {
        return(x[-nrow(x),])
      }
      else {
        return(x)
      }
    })
  ) %>%
  unnest(data) %>%
  add_count(model, category, predicate_id, argument_id) %>%
  mutate(
    group = case_when(
      typicality_rank <= n/2 ~ "typical",
      TRUE ~ "atypical"
    )
  )

pairing_human <- human_groups %>%
  filter(group == "typical") %>%
  # inner_join(experiment_results %>% select(-typicality_rank, -typicality, strength1 = strength)) %>%
  distinct(category, item1 = item) %>%
  group_by(category) %>%
  nest(item1 = -group_cols()) %>%
  inner_join(
    human_groups %>%
      filter(group == "atypical") %>%
      # inner_join(experiment_results %>% select(-typicality_rank, -typicality, strength2 = strength)) %>%
      distinct(category, item2 = item) %>%
      group_by(category) %>%
      nest(item2 = -group_cols())
  )

pairing_model <- model_groups %>%
  filter(group == "typical") %>%
  # inner_join(experiment_results %>% select(-typicality_rank, -typicality, strength1 = strength)) %>%
  select(-typicality_rank, -n, -group) %>%
  rename(strength1 = strength, item1 = item) %>%
  group_by(category, model, predicate_id, argument_id) %>%
  nest(item1 = -group_cols()) %>%
  inner_join(
    model_groups %>%
      filter(group == "atypical") %>%
      # inner_join(experiment_results %>% select(-typicality_rank, -typicality, strength2 = strength)) %>%
      select(-typicality_rank, -n, -group) %>%
      rename(strength2 = strength, item2 = item) %>%
      group_by(category, model, predicate_id, argument_id) %>%
      nest(item2 = -group_cols())
  )

# pairing_model <- model_groups %>%
#   rename(strength1 = strength) %>%
#   filter(group == "typical") %>%
#   select(-group, -n, -typicality_rank) %>%
#   rename(item1 = item) %>%
#   group_by(model, category, predicate_id, argument_id) %>%
#   nest(item1 = -group_cols()) %>%
#   inner_join(
#     model_groups %>%
#       rename(strength2 = strength) %>%
#       filter(group == "atypical") %>%
#       select(-group, -n, -typicality_rank) %>%
#       rename(item2 = item) %>%
#       group_by(model, category, predicate_id, argument_id) %>%
#       nest(item2 = -group_cols())
#   )
```

## Avg. Typicality Ratings for Humans vs Models
```{r}
scale_minmax <- function(x, range = c(0, 1)) {
  scaled = (x - min(x))/(max(x) - min(x)) * (range[2] - range[1]) + range[1]
  return(scaled)
}

experiment_results_corrected <- corrected %>%
  inner_join(typicality_ratings %>% select(model, category, taxonomic = score, rating) %>% group_by(model, category) %>% mutate(item_id = row_number())) %>%
  ungroup() %>%
  group_by(model, category, predicate_id, argument_id) %>%
  mutate(
    strength = scale_minmax(corrected_score),
    rating = scale_minmax(-rating),
    taxonomic = scale_minmax(taxonomic)
  )
  

paired_comparison <- pairing_human %>%
  unnest(c(item1, item2)) %>%
  ungroup() %>%
    inner_join(experiment_results_corrected %>% select(model, category, predicate_id, argument_id, item1 = item, strength1 = strength, rating1 = rating, taxonomic1 = taxonomic)) %>% inner_join(experiment_results_corrected %>% select(model, category, predicate_id, argument_id, item2 = item, strength2 = strength, rating2 = rating, taxonomic2= taxonomic)) %>%
  group_by(model, category, item1, item2) %>%
  summarize(
    strength1 = mean(strength1),
    strength2 = mean(strength2),
    rating1 = mean(rating1),
    rating2 = mean(rating2),
    taxonomic1 = mean(taxonomic1),
    taxonomic2 = mean(taxonomic2)
  )

paired_comparison %>%
  select(model, category, item1, item2, taxonomic1, taxonomic2, strength1, strength2) %>%
  group_by(model) %>%
  nest() %>%
  mutate(
    tests = map(data, function(x) {
      n_success_t = sum(x$taxonomic1 > x$taxonomic2)
      t <- binom.test(n_success_t, nrow(x)) %>%
        glance() %>%
        select(t_success = estimate, t_pvalue = p.value)
      
      n_success_as = sum(x$strength1 > x$strength2)
      as <- binom.test(n_success_as, nrow(x)) %>%
        glance() %>%
        select(as_success = estimate, as_pvalue = p.value)
      bind_cols(t, as)
    })
  ) %>%
  select(-data) %>%
  unnest(tests)

run_btests <- function() {
  result <- pairing_human %>%
  mutate(
      item2 = map(item2, function(x) {
          rows = sample(nrow(x))
          return(x[rows, ])
      })
  ) %>%
  unnest(c(item1, item2)) %>% 
    ungroup() %>%
    inner_join(experiment_results_corrected %>% select(model, category, predicate_id, argument_id, item1 = item, strength1 = strength, taxonomic1 = taxonomic)) %>% inner_join(experiment_results_corrected %>% select(model, category, predicate_id, argument_id, item2 = item, strength2 = strength, taxonomic2= taxonomic)) %>%
  group_by(model, category, item1, item2) %>%
  summarize(
    strength1 = mean(strength1),
    strength2 = mean(strength2),
    taxonomic1 = mean(taxonomic1),
    taxonomic2 = mean(taxonomic2)
  ) %>%
    ungroup() %>%
    group_by(model) %>%
  nest() %>%
  mutate(
    tests = map(data, function(x) {
      n_success_t = sum(x$taxonomic1 > x$taxonomic2)
      t <- binom.test(n_success_t, nrow(x)) %>%
        glance() %>%
        select(t_success = estimate, t_pvalue = p.value)
      
      n_success_as = sum(x$strength1 > x$strength2)
      as <- binom.test(n_success_as, nrow(x)) %>%
        glance() %>%
        select(as_success = estimate, as_pvalue = p.value)
      bind_cols(t, as)
    })
  ) %>%
  select(-data) %>%
  unnest(tests)
  
  return(result)
}
  
```

```{r}
# real fun
set.seed(1234)
btests <- replicate(100, run_btests(), simplify = FALSE)
```


```{r}
btests %>%
  bind_rows(.id = "run") %>%
  group_by(model) %>%
  summarise(
    t_success = mean(t_success),
    adj_p_t = p.adjust(t_pvalue, "bonferroni"),
    as_success = mean(as_success),
    adj_p_as = p.adjust(as_pvalue, "bonferroni")
  )
```


```{r}
per_model <- paired_comparison %>%
  group_by(model) %>%
  summarize(t1 = mean(taxonomic1), t2 = mean(taxonomic2)) %>%
  pivot_longer(t1:t2) %>%
  mutate(name = case_when(name == "t1" ~ "High Typicality", TRUE ~ "Low Typicality"))

ngram_taxonomic <- pairing_human %>%
  unnest(c(item1, item2)) %>%
  inner_join(corrected %>% ungroup() %>% distinct(category, item, item_id) %>% rename(item1 = item, item_id1 = item_id)) %>%
  inner_join(corrected %>% ungroup() %>% distinct(category, item, item_id) %>% rename(item2 = item, item_id2 = item_id)) %>%
  inner_join(typicality_ratings %>% filter(model == "5gram") %>% select(category, item_id1 = rank, taxonomic1 = score)) %>%
  inner_join(typicality_ratings %>% filter(model == "5gram") %>% select(category, item_id2 = rank, taxonomic2 = score)) %>%
  group_by(category) %>%
  mutate(
    taxonomic1 = scale_minmax(taxonomic1),
    taxonomic2 = scale_minmax(taxonomic2)
  ) %>%
  ungroup() %>%
  summarize(t1 = mean(taxonomic1), t2 = mean(taxonomic2)) %>%
  pivot_longer(t1:t2) %>%
  mutate(name = case_when(name == "t1" ~ "High Typicality", TRUE ~ "Low Typicality"), model = "5gram")

high_low_taxonomic <- bind_rows(
  per_model,
  paired_comparison %>%
    ungroup() %>%
    summarize(t1 = mean(taxonomic1), t2 = mean(taxonomic2)) %>%
    pivot_longer(t1:t2) %>%
    mutate(name = case_when(name == "t1" ~ "High Typicality", TRUE ~ "Low Typicality"), model = "average"),
  ngram_taxonomic,
  paired_comparison %>%
    ungroup() %>%
    summarize(t1 = mean(rating1), t2 = mean(rating2)) %>%
    pivot_longer(t1:t2) %>%
    mutate(name = case_when(name == "t1" ~ "High Typicality", TRUE ~ "Low Typicality"), model = "human")
) %>%
  mutate(
    color = case_when(
      model == "human" ~ "#a20a0a",
      model == "average" ~ "#0278ae",
      model == "5gram" ~ "black",
      TRUE ~ "black"
    ),
    alpha = case_when(
      model == "human" ~ 1,
      model == "average" ~ 1,
      model == "5gram" ~ 1,
      TRUE ~ 0.1
    ),
    size = case_when(
      model == "human" ~ 1.4,
      model == "average" ~ 1.4,
      model == "5gram" ~ 1,
      TRUE ~ 1
    ),
    linetype = case_when(
      model == "human" ~ "solid",
      model == "average" ~ "solid",
      model == "5gram" ~ "dotdash",
      TRUE ~ "solid"
    ),
    name = factor(name, levels = c("Low Typicality", "High Typicality"))
  )

high_low_taxonomic %>%
  ggplot(aes(name, value, group = model, color = color, alpha = alpha, linetype = linetype)) +
  geom_point(size = 3) +
  geom_line(aes(size = size)) +
  annotate("text", label = "5gram", x = "High Typicality", y = 0.4, size = 6.5) +
  annotate("text", label = "Human", x = "High Typicality", y = 0.88, size = 6.5, color = "#a20a0a") +
  annotate("text", label = "Avg.\nof LMs", x = 0.85, y = 0.6, size = 6.5, color = "#0278ae") +
  scale_y_continuous(limits = c(0,1), expand = c(0,0.01)) + 
  scale_x_discrete(expand = c(0,0.3)) +
  scale_color_identity() + 
  scale_linetype_identity() + 
  scale_alpha_identity() +
  scale_size_identity() +
  labs(
    y = "Rating (Scaled)",
    x = ""
  ) +
  theme_bw(base_size = 20) +
  theme(
    panel.grid = element_blank(),
    axis.title.x = element_blank()
  )

ggsave(here::here("paper/binnedtaxonomic.pdf"), height = 4.4, width = 5.5)

high_low_induction <- bind_rows(
  paired_comparison %>%
    group_by(model) %>%
    summarize(t1 = mean(strength1), t2 = mean(strength2)) %>%
    pivot_longer(t1:t2) %>%
    mutate(name = case_when(name == "t1" ~ "High Typicality", TRUE ~ "Low Typicality")),
  paired_comparison %>%
    ungroup() %>%
    summarize(t1 = mean(strength1), t2 = mean(strength2)) %>%
    pivot_longer(t1:t2) %>%
    mutate(name = case_when(name == "t1" ~ "High Typicality", TRUE ~ "Low Typicality"), model = "average"),
  paired_comparison %>%
    ungroup() %>%
    summarize(t1 = mean(rating1), t2 = mean(rating2)) %>%
    pivot_longer(t1:t2) %>%
    mutate(name = case_when(name == "t1" ~ "High Typicality", TRUE ~ "Low Typicality"), model = "human")
) %>%
  mutate(
    color = case_when(
      model == "human" ~ "#a20a0a",
      model == "average" ~ "#0278ae",
      model == "5gram" ~ "black",
      TRUE ~ "black"
    ),
    alpha = case_when(
      model == "human" ~ 1,
      model == "average" ~ 1,
      model == "5gram" ~ 1,
      TRUE ~ 0.1
    ),
    size = case_when(
      model == "human" ~ 1.4,
      model == "average" ~ 1.4,
      model == "5gram" ~ 1,
      TRUE ~ 1
    ),
    linetype = case_when(
      model == "human" ~ "solid",
      model == "average" ~ "solid",
      model == "5gram" ~ "dotdash",
      TRUE ~ "solid"
    ),
    name = factor(name, levels = c("Low Typicality", "High Typicality"))
  )

high_low_induction %>%
  ggplot(aes(name, value, group = model, color = color, alpha = alpha, linetype = linetype)) +
  geom_point(size = 3) +
  geom_line(aes(size = size)) +
  # annotate("text", label = "5gram", x = "High Typicality", y = 0.4, size = 6.5) +
  annotate("text", label = "Human", x = "High Typicality", y = 0.88, size = 6, color = "#a20a0a") +
  annotate("text", label = "Avg.\nof LMs", x = 0.85, y = 0.5, size = 6, color = "#0278ae") +
  scale_y_continuous(limits = c(0,1), expand = c(0,0.01)) + 
  scale_x_discrete(expand = c(0,0.3)) +
  scale_color_identity() + 
  scale_linetype_identity() + 
  scale_alpha_identity() +
  scale_size_identity() +
  labs(
    y = "Rating (Scaled)",
    x = ""
  ) +
  theme_bw(base_size = 18) +
  theme(
    panel.grid = element_blank(),
    axis.title.x = element_blank()
  )

ggsave(here::here("paper/binnedinduction.pdf"), height = 5.227, width = 6.927)

```


```{r}
run_pairing <- function(mode) {
  if(mode == "human"){
    result <- pairing_human %>%
    mutate(
        # item1 = map(item1, function(x) {
        #     rows = sample(nrow(x))
        #     return(x[rows, ])
        # }),
        item2 = map(item2, function(x) {
            rows = sample(nrow(x))
            return(x[rows, ])
        })
    ) %>%
    unnest(c(item1, item2)) %>% ungroup() %>%
      inner_join(experiment_results %>% select(model, category, predicate_id, argument_id, item1 = item, strength1 = strength)) %>%
      inner_join(experiment_results %>% select(model, category, predicate_id, argument_id, item2 = item, strength2 = strength))
  }
  else if(mode == "model") {
    result <- pairing_model %>%
    mutate(
        # item1 = map(item1, function(x) {
        #     rows = sample(nrow(x))
        #     return(x[rows, ])
        # }),
        item2 = map(item2, function(x) {
            rows = sample(nrow(x))
            return(x[rows, ])
        })
    ) %>%
    unnest(c(item1, item2)) %>% ungroup()
  } else {
    return("error")
  }
  return(result)
}
```

```{r}
set.seed(1234)
# this will be fun.
replicates_human <- replicate(100, run_pairing("human"), simplify = FALSE)
replicates_model <- replicate(100, run_pairing("model"), simplify = FALSE)
```

## Models vs Human Typicality Judgements

```{r}
replicates_human %>%
  map(function(x) {
    x %>%
      group_by(model, category) %>%
      summarize(
        alignment = mean(strength1 > strength2)
      )
  }) %>%
  bind_rows(.id = "run") %>%
  group_by(model) %>%
  summarize(
    se = 1.96 * plotrix::std.error(alignment), 
    alignment = mean(alignment)
  ) %>%
  inner_join(model_meta) %>%
  mutate(model = factor(model, levels = levels), short = factor(short, levels = shortlevels)) %>%
  ggplot(aes(short, alignment, color = color, fill = color)) +
  geom_col() +
  geom_hline(yintercept = 0.5, linetype = 2) +
  # geom_text(aes(y = 0.95, label = model), color = "black", angle = 90, hjust = "right", vjust = "center") +
  # facet_wrap(~category) +
  # scale_color_identity(guide = "legend", name = "Model", aesthetics = c("color", "fill"), labels = levels, breaks = colors) +
  scale_color_identity(aesthetics = c("color", "fill")) +
  scale_y_continuous(limits = c(0, 1.0), labels = scales::percent_format(), expand = c(0,0.01)) +
  labs(
    x = "Model",
    y = "Premise Typicality"
  ) +
  theme(
    legend.position = "top",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_text(angle = 20, vjust = 0.8),
    panel.background = element_rect(fill = "transparent"), # bg of the panel
    plot.background = element_rect(fill = "transparent", color = NA), # bg of the plot
    legend.background = element_rect(fill = "transparent"), # get rid of legend bg
    legend.box.background = element_rect(fill = "transparent")
  )

ggsave("paper/premisetypicalityoverall.pdf", height = 5, width = 10, bg = "transparent")
```

```{r}
replicates_model %>%
  map(function(x) {
    x %>%
      group_by(model) %>%
      summarize(
        alignment = mean(strength1 > strength2)
      )
  }) %>%
  bind_rows(.id = "run") %>%
  group_by(model) %>%
  summarize(
    se = 1.96 * plotrix::std.error(alignment), 
    alignment = mean(alignment)
  ) %>%
  inner_join(model_meta) %>%
  mutate(model = factor(model, levels = levels)) %>%
  ggplot(aes(model, alignment, color = color, fill = color)) +
  geom_col() +
  geom_hline(yintercept = 0.5, linetype = 2) +
  # geom_text(aes(y = 0.95, label = model), color = "black", angle = 90, hjust = "right", vjust = "center") +
  # facet_wrap(~category) +
  # scale_color_identity(guide = "legend", name = "Model", aesthetics = c("color", "fill"), labels = levels, breaks = colors) +
  # scale_fill_identity() +
  scale_color_identity(aesthetics = c("color", "fill")) +
  scale_y_continuous(limits = c(0, 1.0), labels = scales::percent_format()) +
  labs(
    x = "Model",
    y = "Premise Typicality"
  ) +
  theme(
    legend.position = "top",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_text(angle = 20, vjust = 0.8),
    # axis.ticks.x = element_blank(),
    # axis.title.x = element_blank()
  )
```

```{r}
induction %>%
  group_by(model, predicate_id, argument_id, category) %>% mutate(item_id = row_number()) %>% ungroup() %>%
  inner_join(typicality_ratings %>% select(model, category, model_score = score, rating) %>% group_by(model, category) %>% mutate(item_id = row_number())) %>%
  group_by(model, category, predicate_id, argument_id) %>%
  mutate(score = (score - min(score))/(max(score) - min(score))) %>%
  ungroup() %>%
  group_by(model, category, item, item_id) %>%
  summarize(
    score = mean(score),
    rating = mean(rating)
  ) %>%
  group_by(model) %>%
  nest() %>%
  mutate(
    cor = map(data, function(x) {
      cor.test(-x$score, x$rating) %>% tidy()
    })
  ) %>%
  select(-data) %>%
  unnest(cor) %>% View("induction cor")
```


```{r}

```

